{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook imports the motion data collected on board the R/V Akademik Tryoshnikov during the Antarctic Circumnavigation Expedition (ACE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas\n",
    "import datetime\n",
    "import time\n",
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up pandas display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas.set_option('display.max_columns', 100)\n",
    "pandas.set_option('display.max_rows', 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the hard-coded variables to import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_input_data_folder = \"/home/jen/projects/ace_data_management/ship_data/motion_data/test\"\n",
    "output_data_folder = \"/home/jen/projects/ace_data_management/wip/motion_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the set of raw motion data files in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_input_txt_files(input_data_folder):\n",
    "    \"\"\"Create a list of files in a directory and put them into a list.\"\"\"\n",
    "    \n",
    "    list_data_files = []\n",
    "    \n",
    "    os.chdir(input_data_folder)\n",
    "    directory_path = os.getcwd()\n",
    "    \n",
    "    for filename in os.listdir(input_data_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            fullpath = directory_path + \"/\" + filename\n",
    "            list_data_files.append(fullpath)\n",
    "    \n",
    "    return list_data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check file headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all the files have the same header so that when creating the pandas data frame, the header can always be excluded. \n",
    "\n",
    "The header consists of 5 lines - an example below: \n",
    "\n",
    "Serial n° Hydrins :     PH-497\n",
    "\n",
    "Created the :   07/01/2017              Time :  11h 10m 39.822s\n",
    "\n",
    "Sample period : 1000 ms\n",
    "\n",
    "Pc - HH:MM:SS.SSS       Hydrins - HH:MM:SS.SSS  Heading (°)     Roll (°)        Pitch (°)       Heading std. dev. (°)   Roll std. dev. (°)      Pitch std. dev. (°)     North speed (m/s)       East speed (m/s)        Vert. speed (m/s)       Speed norm (knots)      North speed std. dev. (m/s)     East speed std. dev. (m/s)      Vert. speed std. dev. (m/s)     Latitude (°)    Longitude (°)   Altitude (m)    Latitude std. dev. (m)  Longitude std. dev. (m) Altitude std. dev. (m)  Zone I  Zone C  UTM North (m)   UTM East (m)    UTM altitude  (m)       High level status       System status 1 System status 2 Algo status 1   Algo status 2   GPS - Latitude (°)      GPS - Longitude (°)     GPS - Altitude (m)      GPS - Mode      GPS - Time      Manual GPS - Latitude (°)       Manual GPS - Longitude (°)      Manual GPS - Altitude (m)       Manual GPS - Latitude std. dev. Manual GPS - Longitude std. dev.        Manual GPS - Altitude std. dev.\n",
    "\n",
    "Note that we will need the date from the second line in the header and the column header will be checked and set as the column headers in the data frame. The length of the header in the files is not the same as the number of data columns so this also needs to be checked. \n",
    "\n",
    "The header contains 43 \"columns\", the last one of which is empty. Whereas each line of data contains 45 columns, also with the last one empty. Therefore there are two columns which are not labelled and three in total which do not have a header (including the unnamed ones). These unnamed columns have been labelled as unknown1, unknown2 and unknown3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected header is defined in order to compare that within each file (note that this ignores the first four lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected_header = ['Pc - HH:MM:SS.SSS', 'Hydrins - HH:MM:SS.SSS', 'Heading (°)', 'Roll (°)', 'Pitch (°)', 'Heading std. dev. (°)', 'Roll std. dev. (°)', 'Pitch std. dev. (°)', 'North speed (m/s)', 'East speed (m/s)', 'Vert. speed (m/s)', 'Speed norm (knots)', 'North speed std. dev. (m/s)', 'East speed std. dev. (m/s)', 'Vert. speed std. dev. (m/s)', 'Latitude (°)', 'Longitude (°)', 'Altitude (m)', 'Latitude std. dev. (m)', 'Longitude std. dev. (m)', 'Altitude std. dev. (m)', 'Zone I', 'Zone C', 'UTM North (m)', 'UTM East (m)', 'UTM altitude  (m)', 'High level status', 'System status 1', 'System status 2', 'Algo status 1', 'Algo status 2', 'GPS - Latitude (°)', 'GPS - Longitude (°)', 'GPS - Altitude (m)', 'GPS - Mode', 'GPS - Time', 'Manual GPS - Latitude (°)', 'Manual GPS - Longitude (°)', 'Manual GPS - Altitude (m)', 'Manual GPS - Latitude std. dev.', 'Manual GPS - Longitude std. dev.', 'Manual GPS - Altitude std. dev.', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(expected_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_file_header(list_data_files): \n",
    "    \"\"\"Check that the header for each of the files in the list is as expected. If it is correct, add one to the number of correct headers that is output and if incorrect, output the filename and a copy of the header line. Also add one to the number of incorrect headers.\"\"\"\n",
    "    \n",
    "    correct_headers = 0\n",
    "    incorrect_headers = 0\n",
    "    total_number_files = len(list_data_files)\n",
    "    \n",
    "    total_number_headers = 0\n",
    "    for file in list_data_files:\n",
    "        total_number_headers += 1\n",
    "        print(\"Checking the header of file\", total_number_headers, \"out of\", total_number_files)\n",
    "        \n",
    "        with open(file, 'r', encoding=\"ISO-8859-1\") as csvfile: # encoding that of original files - required because of degrees characters\n",
    "            contents = csv.reader(csvfile, delimiter='\\t')\n",
    "        \n",
    "            line_number = 0\n",
    "        \n",
    "            for line in contents:\n",
    "                if line_number == 4:\n",
    "                    if line != expected_header:\n",
    "                        print(\"Wrong header: \", file, \"  \", line)\n",
    "                        incorrect_headers += 1\n",
    "                    else:\n",
    "                        correct_headers += 1\n",
    "                \n",
    "                line_number += 1\n",
    "    \n",
    "    total_no_files = correct_headers + incorrect_headers\n",
    "    \n",
    "    print(\"Correct headers: \", correct_headers)\n",
    "    print(\"Incorrect headers: \", incorrect_headers)\n",
    "    print(\"Total number of files: \", total_no_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the date of the data collection and read the data into a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read header of file to extract the date. This is important because otherwise, when the data are read into a data frame, they will not have a date associated with them.\n",
    "\n",
    "Note that the line within the header that contains the date is, for example: \n",
    "\n",
    "Created the : 07/01/2017 Time : 11h 10m 39.822s\n",
    "\n",
    "However not all the data may be collected on that particular date. Each data file contains 1000 ms of data, so if the file begins just before midnight then it is possible that it may end with data from the following day. This needs to be taken into account when assigning the date to the data. \n",
    "\n",
    "So far only the PC date has been added as a separate field (pc_date_utc). it is possible that the other time fields have a different date associated with them (Hydrins and GPS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_motion_file_date(filename):\n",
    "    \"\"\"Read a file and get the date from the second line in the header. Output the date in the given format.\"\"\"\n",
    "    \n",
    "    with open(filename, 'r', encoding=\"ISO-8859-1\") as data_file: # need encoding because of degree characters\n",
    "        data_file.readline() # skips first line\n",
    "        date_line = data_file.readline()\n",
    "        \n",
    "        date = date_line.split(\"\\t\")[1]\n",
    "        \n",
    "        return datetime.datetime.strptime(date, \"%d/%m/%Y\")\n",
    "    \n",
    "    \n",
    "def optimise_line(line):\n",
    "    \"\"\"Convert the values in a line of data that look like numbers, to floats (to optimise the memory usage and make the next stage more efficient). If the value is not a number, then leave it in its original format.\"\"\"\n",
    "    for i, value in enumerate(line):\n",
    "        try:\n",
    "            line[i] = float(line[i])\n",
    "        except ValueError:\n",
    "            pass  \n",
    "        \n",
    "    \n",
    "#def data_to_list(list_data_files, rows_of_data):\n",
    "#    \"\"\"Read files from a list of files, get the date from each file, then append the date to each line within the file as the line of data is read into a list. Output a list of data from all of the files.\"\"\"\n",
    "#    \n",
    "#    total_number_files = len(list_data_files)\n",
    "#    \n",
    "#    total_rows = 0\n",
    "#    number_files = 0\n",
    "#    for file in list_data_files:\n",
    "#        number_files += 1\n",
    "#        print(\"Processing file\", number_files, \" of \", total_number_files)\n",
    "#        \n",
    "#        date_of_file = read_motion_file_date(file)\n",
    "#    \n",
    "#        with open(file, 'r', encoding=\"ISO-8859-1\") as data_file:\n",
    "#            contents = csv.reader(data_file, delimiter='\\t')\n",
    "#            for i in range(5): \n",
    "#                next(contents)\n",
    "#                    \n",
    "#            previous_time_pc = None\n",
    "#            row_count = 0\n",
    "#            for line in contents:\n",
    "#\n",
    "#                time_now_pc = time.strptime(line[0], \"%H:%M:%S.%f\")\n",
    "#\n",
    "#                if previous_time_pc is None: \n",
    "#                    current_date = date_of_file\n",
    "#                elif time_now_pc < previous_time_pc:\n",
    "#                    current_date = current_date + datetime.timedelta(days=1)\n",
    "#                \n",
    "#                line.insert(0, current_date.strftime(\"%Y-%m-%d\"))\n",
    "#                previous_time_pc = time_now_pc\n",
    "#                optimise_line(line)\n",
    "#                rows_of_data.append(line) \n",
    "#\n",
    "#                row_count += 1\n",
    "#            \n",
    "#            print(file, \"contains a total number of rows:\", row_count)\n",
    "#        \n",
    "#        total_rows += row_count\n",
    "#    #print(\"\\nTotal number of rows expected in dataframe:\", total_rows) # this is taking too long\n",
    "#                \n",
    "#    return rows_of_data   \n",
    "\n",
    "def data_to_list(filename, rows_of_data):\n",
    "    \"\"\"Read files from a list of files, get the date from each file, then append the date to each line within the file as the line of data is read into a list. Output a list of data from all of the files.\"\"\"\n",
    "\n",
    "    date_of_file = read_motion_file_date(filename)\n",
    "\n",
    "    with open(filename, 'r', encoding=\"ISO-8859-1\") as data_file:\n",
    "        contents = csv.reader(data_file, delimiter='\\t')\n",
    "        for i in range(5):\n",
    "            next(contents)\n",
    "\n",
    "        pc_previous_time = None\n",
    "        row_count = 0\n",
    "        for line in contents:\n",
    "\n",
    "            pc_time_now = time.strptime(line[0], \"%H:%M:%S.%f\")\n",
    "            #print(\"PC\", line[0], pc_time_now)\n",
    "            hydrins_time_now = time.strptime(line[1], \"%H:%M:%S.%f\")\n",
    "            #print(\"Hydrins\", line[1], hydrins_time_now)\n",
    "            gps_time_now = time.strptime(line[35], \"%H:%M:%S.%f\")\n",
    "            #print(\"GPS\", line[35], gps_time_now)\n",
    "            \n",
    "            if pc_previous_time is None:\n",
    "                pc_current_date = date_of_file\n",
    "            elif pc_time_now < pc_previous_time:\n",
    "                pc_current_date = pc_current_date + datetime.timedelta(days=1)\n",
    "            #print(\"pc current date\", pc_current_date)\n",
    "\n",
    "            line.insert(0, pc_current_date.strftime(\"%Y-%m-%d\"))\n",
    "            \n",
    "            if hydrins_time_now >= time.strptime(\"23:00:00.0\", \"%H:%M:%S.%f\") and pc_time_now <= time.strptime(\"01:00:00.0\", \"%H:%M:%S.%f\"):\n",
    "                hydrins_current_date = pc_current_date - datetime.timedelta(days=1)\n",
    "            elif pc_time_now >= time.strptime(\"23:00:00.0\", \"%H:%M:%S.%f\") and hydrins_time_now <= time.strptime(\"01:00:00.0\", \"%H:%M:%S.%f\"):\n",
    "                hydrins_current_date = pc_current_date + datetime.timedelta(days=1)\n",
    "            else:\n",
    "                hydrins_current_date = pc_current_date\n",
    "                \n",
    "                #hydrins_time_now_t = time.mktime(hydrins_time_now)\n",
    "                #pc_time_now_t = time.mktime(pc_time_now)\n",
    "                #if abs(pc_time_now_t-hydrins_time_now_t) > 7200:\n",
    "                #    raise \"Failed\"\n",
    "                # ime.strptime(pc_time_now) - time.strptime(hydrins_time_now)) <= time.strptime(\"02:00:00.0\", \"%H:%M:%S.%f\"):\n",
    "                #hydrins_current_date = pc_current_date\n",
    "\n",
    "            #print(\"hydrins current date\", hydrins_current_date)\n",
    "                \n",
    "            line.insert(2, hydrins_current_date.strftime(\"%Y-%m-%d\"))\n",
    "            \n",
    "            if gps_time_now >= time.strptime(\"23:00:00.0\", \"%H:%M:%S.%f\") and pc_time_now <= time.strptime(\"01:00:00.0\", \"%H:%M:%S.%f\"):\n",
    "                gps_current_date = pc_current_date - datetime.timedelta(days=1)\n",
    "            elif pc_time_now >= time.strptime(\"23:00:00.0\", \"%H:%M:%S.%f\") and gps_time_now <= time.strptime(\"01:00:00.0\", \"%H:%M:%S.%f\"):\n",
    "                gps_current_date = pc_current_date + datetime.timedelta(days=1)\n",
    "            else:\n",
    "                gps_current_date = pc_current_date\n",
    "            \n",
    "            #print(\"gps current date\", gps_current_date)\n",
    "\n",
    "            line.insert(37, gps_current_date.strftime(\"%Y-%m-%d\"))\n",
    "            #print(line)\n",
    "            pc_previous_time = pc_time_now\n",
    "            optimise_line(line)\n",
    "            rows_of_data.append(line)\n",
    "\n",
    "            row_count += 1\n",
    "\n",
    "        print(filename, \"contains a total number of rows:\", row_count)\n",
    "\n",
    "    return rows_of_data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the column headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the header for the columns so that this can be assigned to the data frame. \n",
    "\n",
    "Currently this is listed in a csv file so that it can be changed and reimported as necessary. \n",
    "\n",
    "The data contains more columns than are listed in the header in the data files, so we need to ensure that these are assigned correctly in the pandas data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def define_column_headers(header_file):\n",
    "    \"\"\"Import a list of column headers from a csv file and output them as a list to be ready to be set as the column headers for a pandas data frame.\"\"\"\n",
    "\n",
    "    header = []\n",
    "\n",
    "    with open(header_file) as headerfile:\n",
    "        contents = csv.reader(headerfile)\n",
    "        header_list = list(contents)\n",
    "    \n",
    "        for item in header_list: \n",
    "            header.append(item[0])\n",
    "            \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header_file = \"/home/jen/projects/ace_data_management/wip/motion_data/file_header.csv\"\n",
    "\n",
    "header = define_column_headers(header_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data frame and import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all of the data into a data frame using pandas, which has the column headers as defined from the csv file of column headers.\n",
    "\n",
    "The date needs to be extracted from the file header depending on the time of the data point, and inserted as an additional column in the data frame. \n",
    "\n",
    "Data will be put into a list first, so that the date can be added, then this list of data will be added to the pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_to_dataframe(rows_of_data, dataframe, header):\n",
    "    \"\"\"Import a list of data into a pandas dataframe and use a list of column names as the header.\"\"\"\n",
    "    \n",
    "    dataframe = dataframe.append(pandas.DataFrame(rows_of_data, columns = header), ignore_index = True)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimise memory usage in the dataframe by converting float64 to float32 (uses less bytes per digit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The code below was taken from https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65 and is used to convert the datatype to one that uses less memory.\n",
    "\n",
    "def reduce_memory_usage(props):\n",
    "    \"\"\"Takes a dataframe and converts the data type of each float to float64 (oroignally did it to float32 but wanted more dp for lat and lon, reducing the memory usage.\"\"\"\n",
    "    \n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if props[col].dtype != object:  # Exclude strings\n",
    "            \n",
    "            # Print current column type\n",
    "            #print(\"******************************\")\n",
    "            #print(\"Column: \",col)\n",
    "            #print(\"dtype before: \",props[col].dtype)\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = (props[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float64) # changed this from float32 to avoid losing the accuracy of the latitude and longitude\n",
    "            \n",
    "            # Print new column type\n",
    "            #print(\"dtype after: \",props[col].dtype)\n",
    "            #print(\"******************************\")\n",
    "            \n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return props, NAlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the data into csv files with one file per date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_daily_files(dataframe, output_data_folder):\n",
    "    \"\"\"Output a pandas dataframe to a files where each file contains one day's worth of data. Output the data file to the output data folder.\"\"\"\n",
    "    \n",
    "    output_filename_base = 'ace_hydrins_'\n",
    "\n",
    "    date_group = dataframe.groupby('pc_date_utc')\n",
    "    print(\"Aggregated groups by date with counts:\")\n",
    "    print(dataframe.groupby('pc_date_utc').size())\n",
    "    print(\"\\nTotal number of records:\")\n",
    "    print(dataframe.groupby('pc_date_utc').size().sum())\n",
    "    \n",
    "    for date in date_group.groups:\n",
    "        date_formatted = datetime.datetime.strptime(date, \"%Y-%m-%d\")    \n",
    "\n",
    "        date_string = date_formatted.strftime('%Y%m%d')\n",
    "       \n",
    "        output_filename = output_data_folder + output_filename_base + date_string + \".csv\"\n",
    "        \n",
    "        date_group.get_group(date).to_csv(output_filename, sep=\",\", header=True, index=False)\n",
    "        print(date, \"file created\") # TODO put a better check here that the file exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a test list of files and test the import. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list_data_files = ['/media/jen/SAMSUNG/motion_data/ACE_Bremen-2_300.txt', '/media/jen/SAMSUNG/motion_data/ACE_Bremen-2_301.txt', '/media/jen/SAMSUNG/motion_data/ACE_Bremen-2_302.txt', '/media/jen/SAMSUNG/motion_data/ACE_Bremen-2_303.txt', '/media/jen/SAMSUNG/motion_data/ACE_Bremen-2_304.txt', '/media/jen/SAMSUNG/motion_data/ACE_Bremen-2_305.txt', '/media/jen/SAMSUNG/motion_data/ACE_Bremen-2_306.txt', '/media/jen/SAMSUNG/motion_data/ACE_Bremen-2_307.txt', '/media/jen/SAMSUNG/motion_data/ACE_Bremen-2_308.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(\"Checking headers of files\")\n",
    "#print(\"\\n\")\n",
    "\n",
    "#check_file_header(test_list_data_files)\n",
    "\n",
    "#print(\"Reading files and creating data frame\")\n",
    "\n",
    "#motiondf = pandas.DataFrame(columns = header)\n",
    "\n",
    "#rows_of_data = list()    \n",
    "\n",
    "#rows_of_data = data_to_list(test_list_data_files, rows_of_data)\n",
    "\n",
    "#motiondf = data_to_dataframe(rows_of_data, motiondf, header)\n",
    "\n",
    "#print(\"The dataframe has\", len(motiondf), \"rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#motiondf.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test outputting the files (one per day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#output_daily_files(motiondf, output_data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the list of data files and import them into the dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_motion_data_files = get_input_txt_files(test_input_data_folder)\n",
    "print(len(list_motion_data_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the headers of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Checking headers of files\")\n",
    "print(\"\\n\")\n",
    "\n",
    "t1_start = time.perf_counter()\n",
    "t2_start = time.process_time()\n",
    "\n",
    "check_file_header(list_motion_data_files)\n",
    "\n",
    "t1_stop = time.perf_counter()\n",
    "t2_stop = time.process_time()\n",
    "\n",
    "elapsed_time_headers = t1_stop - t1_start\n",
    "cpu_processing_time_header = t2_stop - t2_start\n",
    "\n",
    "print(\"Elapsed time headers:\", elapsed_time_headers, \"secs\")\n",
    "print(\"CPU processing time headers:\", cpu_processing_time_header, \"secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data into a list, adding the date to the data in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(\"Reading files and creating data frame\")\n",
    "#motiondf = pandas.DataFrame(columns = header)\n",
    "\n",
    "#rows_of_data = list()    \n",
    "\n",
    "#t1_start = time.perf_counter()\n",
    "#t2_start = time.process_time()\n",
    "\n",
    "#rows_of_data = data_to_list(list_motion_data_files, rows_of_data)\n",
    "\n",
    "#t1_stop = time.perf_counter()\n",
    "#t2_stop = time.process_time()\n",
    "\n",
    "#elapsed_time_date_to_rows = t1_stop - t1_start\n",
    "#cpu_processing_time_date_to_rows = t2_stop - t2_start\n",
    "\n",
    "#print(\"Elapsed time date to rows:\", elapsed_time_date_to_rows, \"secs\")\n",
    "#print(\"CPU processing time date to rows:\", cpu_processing_time_date_to_rows, \"secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(list_motion_data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows_of_data = list()\n",
    "motiondf = pandas.DataFrame(columns = header)\n",
    "\n",
    "t1_start = time.perf_counter()\n",
    "t2_start = time.process_time()\n",
    "\n",
    "no_files_processed = 0\n",
    "for filename in list_motion_data_files:\n",
    "    print(filename)\n",
    "    total_no_files = len(list_motion_data_files)\n",
    "    \n",
    "    data_to_list(filename, rows_of_data)\n",
    "    motiondf = data_to_dataframe(rows_of_data, motiondf, header)\n",
    "    rows_of_data = list()\n",
    "    \n",
    "    reduce_memory_usage(motiondf)\n",
    "    \n",
    "    no_files_processed += 1\n",
    "    print(\"Processed\", no_files_processed, \"out of\", total_no_files)\n",
    "    \n",
    "t1_stop = time.perf_counter()\n",
    "t2_stop = time.process_time()\n",
    "    \n",
    "elapsed_time = t1_stop - t1_start\n",
    "cpu_processing_time = t2_stop - t2_start\n",
    "\n",
    "print(\"Elapsed time:\", elapsed_time, \"secs\")\n",
    "print(\"CPU processing time:\", cpu_processing_time, \"secs\")\n",
    "\n",
    "print(\"\\n******************************\\n\")\n",
    "motiondf.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN TO HERE then pickle then output data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the list of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(len(rows_of_data))\n",
    "#type(rows_of_data)\n",
    "#print(rows_of_data[0:5])\n",
    "#print(len(rows_of_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the list of data to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#t1_start = time.perf_counter()\n",
    "#t2_start = time.process_time()\n",
    "\n",
    "#motiondf = data_to_dataframe(rows_of_data, motiondf, header)\n",
    "\n",
    "#print(\"The dataframe has\", len(motiondf), \"rows\\n\")\n",
    "\n",
    "#t1_stop = time.perf_counter()\n",
    "#t2_stop = time.process_time()\n",
    "\n",
    "#elapsed_time_dataframe = t1_stop - t1_start\n",
    "#cpu_processing_time_dataframe = t2_stop - t2_start\n",
    "\n",
    "#print(\"Elapsed time dataframe:\", elapsed_time_dataframe, \"secs\")\n",
    "#print(\"CPU processing time dataframe:\", cpu_processing_time_dataframe, \"secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#motiondf.iloc[:5]\n",
    "#motiondf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimise the memory usage of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#motiondf, NAlist = reduce_memory_usage(motiondf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import gc\n",
    "#import sys\n",
    "#sys.getrefcount(rows_of_data)\n",
    "#rows_of_data=None\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the dataframe (output it to a file to remove it from the memory) and check the memory usage again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "motiondf.to_pickle(output_data_folder + \"motiondf.pkl\")\n",
    "#motiondf.memory_usage().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "motiondf.reset_index(drop=True, inplace=True)\n",
    "motiondf.to_pickle(output_data_folder + \"motiondf_noindex.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the data files from the dataframe into daily files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "motiondf = pandas.read_pickle(output_data_folder + 'motiondf_noindex.pkl')\n",
    "\n",
    "t1_start = time.perf_counter()\n",
    "t2_start = time.process_time()\n",
    "\n",
    "output_daily_files(motiondf, output_data_folder)\n",
    "\n",
    "t1_stop = time.perf_counter()\n",
    "t2_stop = time.process_time()\n",
    "\n",
    "elapsed_time_file_output = t1_stop - t1_start\n",
    "cpu_processing_time_file_output = t2_stop - t2_start\n",
    "\n",
    "print(\"Elapsed time file output:\", elapsed_time_file_output, \"secs\")\n",
    "print(\"CPU processing time file output:\", cpu_processing_time_file_output, \"secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the output files have the same number of data rows as the dataframe had per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = \"/home/jen/projects/ace_data_management/wip/motion_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of the output files to check (named ace_hydrins_yyyymmdd.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_files(input_data_folder):\n",
    "    \n",
    "    list_data_files = []\n",
    "    \n",
    "    os.chdir(input_data_folder)\n",
    "    directory_path = os.getcwd()\n",
    "    \n",
    "    for filename in os.listdir(input_data_folder):\n",
    "        if filename.startswith(\"ace_hydrins_\"):\n",
    "            fullpath = directory_path + \"/\" + filename\n",
    "            list_data_files.append(fullpath)\n",
    "    \n",
    "    return list_data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of rows in each file, ignoring the header and output with the date and number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_rows_in_file(list_data_files):\n",
    "\n",
    "    total_rows = 0\n",
    "    for filepath in list_data_files:\n",
    "        filename = os.path.basename(filepath)\n",
    "        filedate = (filename.split('_')[-1]).split('.')[0] \n",
    "\n",
    "        with open(filepath, 'r') as csvfile:\n",
    "            contents = csv.reader(csvfile)\n",
    "            next(contents)\n",
    "\n",
    "            row_count = 0\n",
    "            for line in contents:\n",
    "                row_count += 1\n",
    "\n",
    "            print(filedate, \" \", row_count)\n",
    "        \n",
    "        total_rows += row_count\n",
    "    \n",
    "    print(\"Total number of rows in files: \", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_data_files_to_check = get_input_files(data_folder)\n",
    "\n",
    "check_rows_in_file(list_data_files_to_check)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
