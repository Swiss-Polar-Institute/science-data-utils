{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will look at the GPS and GLONASS data collected on board the Antarctic Circumnavigation Expedition, which forms the cruise track. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data were collected simulataneously from a GPS (Trimble) and GLONASS (based on the bridge) to monitor the track of the ship during the expedition. \n",
    "\n",
    "Both data streams were collected in their raw format with a 1-second resolution. In addition, the Trimble GPS data were fed through a number of other on-board instruments, meaning we also have a \"pre-processed\" version of the data. \n",
    "\n",
    "These data sets were pre-processed on board to convert them from the raw format which is a set of NMEA strings, to a more useable, csv format. This was done by combining both data streams and outputting the csv files in a number of resolutions: 1 second, 1 minute, 5 minute and 1 hour, for the needs of different projects. \n",
    "\n",
    "This notebook has the following aims: \n",
    "\n",
    "1 - check the conversion of the raw data to pre-processed data was correct for the GPS and GLONASS data streams\n",
    "\n",
    "2 - check the integrity of the data itself by doing some basic quality checking for the GPS and GLONASS data streams\n",
    "\n",
    "3 - compare the pre-processed, quality-checked data with the pre-processed data contained in the other data streams (eg. motion data)\n",
    "\n",
    "4 - highlight any areas where the data look to be incorrect for the GPS and GLONASS data streams\n",
    "\n",
    "5- compare the GPS and GLONASS data streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up python and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas\n",
    "import datetime\n",
    "import MySQLdb\n",
    "\n",
    "pandas.set_option('display.max_columns', 100)\n",
    "pandas.set_option('display.max_rows', 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data files from a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_files(input_data_folder):\n",
    "    \n",
    "    list_data_files = []\n",
    "    \n",
    "    os.chdir(input_data_folder)\n",
    "    directory_path = os.getcwd()\n",
    "    \n",
    "    for filename in os.listdir(input_data_folder):\n",
    "        if filename.startswith(\"gpsdata_201\"):\n",
    "            fullpath = directory_path + \"/\" + filename\n",
    "            list_data_files.append(fullpath)\n",
    "    \n",
    "    return list_data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a single test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_file(input_data_folder, filename):\n",
    "    \n",
    "    list_data_files = []\n",
    "    \n",
    "    full_filepath = input_data_folder + filename\n",
    "    list_data_files.append(full_filepath)\n",
    "    \n",
    "    return list_data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data from a database table into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_from_database(query, db_connection):\n",
    "    \n",
    "    dataframe = pandas.read_sql(query, con=db_connection)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read list of files into a single dataframe. \n",
    "\n",
    "Note that the number of rows is likely to be large. Each Trimble GPS daily file has ~ 430,000 rows => 105 files ~ 50,000,000 rows.\n",
    "\n",
    "A note on the columns: each row within the raw data file is preceeded by an NMEA string name, eg. GPGGA which denotes what it contains in terms of variables. Therefore as each NMEA string contains a different number of variables, hence each row in the dataframe will contain a variable number of columns. When loading the data into the dataframe, the columns need a name to overcome this problem (see names = list('abcdefghijklmno' in the code, where the number of letters in the list is the same as the maximum number of variables in an NMEA string). \n",
    "\n",
    "Pandas will be used to get the data from different NMEA strings into different dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_files(list_data_files):\n",
    "    \n",
    "    df_from_each_file = (pandas.read_csv(file, names = list('abcdefghijklmno')) for file in list_data_files) # columns are named as letters at the moment. required because the data has irregular numbers of columns in each row.\n",
    "    concatenated_df = pandas.concat(df_from_each_file, ignore_index=True)\n",
    "    \n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific NMEA strings utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe from a specific NMEA string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_nmea_string_data(nmea_string, dataframe, header):\n",
    "    \n",
    "    nmea_dataframe = dataframe.loc[dataframe['nmea_reference'] == nmea_string]\n",
    "    \n",
    "    print(\"Header length:\",len(header))\n",
    "    print(\"Number columns:\", len(nmea_dataframe.columns))\n",
    "    \n",
    "    if len(nmea_dataframe.columns) > len(header):\n",
    "        nmea_dataframe = nmea_dataframe.iloc[:,0:len(header)]\n",
    "        nmea_dataframe.columns = header\n",
    "    elif len(nmea_dataframe.columns) == len(header):\n",
    "        nmea_dataframe.columns = header\n",
    "            \n",
    "#    if nmea_string == '$GPGGA': # TODO this does not work\n",
    "#        nmea_dataframe['fix_time'] = pandas.to_datetime(nmea_dataframe['fix_time'], '%H%M%S')\n",
    "#    elif nmea_string == '$GPZDA':\n",
    "#        nmea_dataframe['record_time'] = pandas.to_datetime(nmea_dataframe['record_time'], '%H%M%S')\n",
    "#    elif nmea_string == '$GPRMC':\n",
    "#        nmea_dataframe['fix_time'] = pandas.to_datetime(nmea_dataframe['fix_time'], '%H%M%S')\n",
    "#        nmea_dataframe['fix_date'] = pandas.to_datetime(nmea_dataframe['fix_date'], '%d%m%y')\n",
    " \n",
    "        #if nmea_string == '$GPGGA':\n",
    "         #   nmea_dataframe['fix_time'] = datetime.datetime.strptime(nmea_dataframe['fix_time'], format='%H%M%S')\n",
    "    return nmea_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define GPGGA header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpgga_header = ['nmea_reference', 'fix_time', 'latitude', 'latitude_ns', 'longitude', 'longitude_ew',\n",
    "             'fix_quality', 'number_satellites', 'horiz_dilution_of_position','altitude', 'altitude_units', 'geoid_height', 'geoid_height_units',\n",
    "             'unknown', 'checksum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define GPZDA header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpzda_header = ['nmea_reference', 'record_time', 'day', 'month', 'year', 'local_time_zone_hours', 'min_checksum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define GPRMC header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gprmc_header = ['nmea_reference', 'fix_time', 'status', 'latitude', 'latitude_ns', 'longitude', 'longitude_ew', 'speed_over_gound_kts', 'track_angle_degs', 'fix_date', 'magnetic_variation', 'magnetic_variation_ew', 'checksum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert numbers to floats to optimise memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimise_line(line):\n",
    "    \"\"\"Convert the values in a line of data in a list that look like numbers, to floats (to optimise the memory usage and make the next stage more efficient). If the value is not a number, then leave it in its original format.\"\"\"\n",
    "    for i, value in enumerate(line):\n",
    "        try:\n",
    "            line[i] = float(line[i])\n",
    "        except ValueError:\n",
    "            pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimise_dataframe(dataframe):    \n",
    "    \n",
    "    dataframe.info()\n",
    "    print(dataframe[:5])\n",
    "    \n",
    "    cols_float64 = ['latitude', 'longitude', 'record_time']\n",
    "    cols_float32 = ['horiz_dilution_of_position', 'altitude', 'geoid_height']\n",
    "    cols_int = ['id', 'fix_quality', 'number_satellites', 'device_id', 'measureland_qualifier_flags_id', 'day', 'month', 'year']\n",
    "    \n",
    "    #for col in cols_float64: \n",
    "    #    if col in dataframe.columns:\n",
    "    #        dataframe[cols_float64] = dataframe[cols_float64].apply(pandas.to_numeric, errors='ignore')\n",
    "            \n",
    "    for col in cols_float32:        \n",
    "        if col in dataframe.columns:\n",
    "            dataframe[cols_float32] = dataframe[cols_float32].apply(pandas.to_numeric, errors='ignore', downcast='float')\n",
    "    \n",
    "    for col in cols_int:\n",
    "        if col in dataframe.columns:\n",
    "            dataframe[col] = dataframe[col].apply(pandas.to_numeric, errors='ignore', downcast='integer')\n",
    "    \n",
    "    dataframe.info()\n",
    "    print(dataframe[:5])\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimise memory usage in the dataframe by converting float64 to float32 (uses less bytes per digit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The code below was taken from https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65 and is used to convert the datatype to one that uses less memory.\n",
    "\n",
    "def reduce_memory_usage(props):\n",
    "    \"\"\"Takes a dataframe and converts the data type of each float to float32, reducing the memory usage.\"\"\"\n",
    "    \n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if props[col].dtype != object:  # Exclude strings\n",
    "            \n",
    "            # Print current column type\n",
    "            #print(\"******************************\")\n",
    "            #print(\"Column: \",col)\n",
    "            #print(\"dtype before: \",props[col].dtype)\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = (props[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "            \n",
    "            # Print new column type\n",
    "            #print(\"dtype after: \",props[col].dtype)\n",
    "            #print(\"******************************\")\n",
    "            \n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return props, NAlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Join the ZDA and GGA nmea sentences together - how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read ZDA lines of file, line by line, then read next line and append it to the ZDA line.\n",
    "nmea_string = '$GPZDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_to_list(filename, nmea_string):\n",
    "    \"\"\"Read files from a list of files, get the date from each file, then append the date to each line within the file as the line of data is read into a list. Output a list of data from all of the files.\"\"\"\n",
    "\n",
    "    row_of_data = []\n",
    "    with open(filename, 'r') as data_file:\n",
    "        contents = csv.reader(data_file, delimiter=',')\n",
    "        for line in contents:\n",
    "            if line[0] == nmea_string:\n",
    "                row_of_data.append(line)\n",
    "                \n",
    "    return row_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-07937849aedb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrows_of_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrow_of_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmea_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mrows_of_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_of_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-7c7534baad40>\u001b[0m in \u001b[0;36mdata_to_list\u001b[0;34m(filename, nmea_string)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnmea_string\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mrow_of_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mgpgga_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mrow_of_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpgga_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "nmea_string = '$GPZDA'\n",
    "\n",
    "list_files = ['/home/jen/projects/ace_data_management/ship_data/gps_trimble/gpsdata_20170104.log', '/home/jen/projects/ace_data_management/ship_data/gps_trimble/gpsdata_20170105.log']\n",
    "\n",
    "rows_of_data = list()\n",
    "for filename in list_files:\n",
    "    row_of_data = data_to_list(filename, nmea_string)\n",
    "    rows_of_data.append(row_of_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rows_of_data[:5]) #this works but i want a list of lists, not list of list of lists TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimise this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the list to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimise the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Check conversion of raw data to pre-processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimble GPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import an example raw data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_data_folder_trimble = '/home/jen/projects/ace_data_management/ship_data/gps_trimble/'\n",
    "trimble_filename = 'gpsdata_20170104.log'\n",
    "\n",
    "#test_list_trimble_data_files = get_input_file(input_data_folder_trimble, trimble_filename)\n",
    "test_list_trimble_data_files = ['/home/jen/projects/ace_data_management/ship_data/gps_trimble/gpsdata_20170104.log', '/home/jen/projects/ace_data_management/ship_data/gps_trimble/gpsdata_20170105.log']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read raw data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trimble_raw_df = read_files(test_list_trimble_data_files)\n",
    "trimble_raw_df = trimble_raw_df.rename(columns = {'a': 'nmea_reference'})\n",
    "len(trimble_raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the start of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trimble_raw_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put GPGGA data into a separate dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmea_string = '$GPGGA'\n",
    "\n",
    "gpgga_trimble_raw_df = get_nmea_string_data(nmea_string, trimble_raw_df, gpgga_header)\n",
    "gpgga_trimble_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpgga_trimble_raw_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimise the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpgga_trimble_raw_df_opt = optimise_dataframe(gpgga_trimble_raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put GPZDA data into a separate dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmea_string = '$GPZDA'\n",
    "\n",
    "gpzda_trimble_raw_df = get_nmea_string_data(nmea_string, trimble_raw_df, gpzda_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpzda_trimble_raw_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpzda_trimble_raw_df_opt = optimise_dataframe(gpzda_trimble_raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the GPZDA and GPGGA rows into another dataframe so that we have a date/timestamp with each latitude and longitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_trimble = 'select * from ship_data_gpggagpsfix where device_id=63;'\n",
    "\n",
    "db_connection = MySQLdb.connect(host = 'localhost', user = 'ace', passwd = 'ace',db = 'ace2016', port = 3306); \n",
    "\n",
    "gpsdb_df = get_data_from_database(query_gps, db_connection)\n",
    "gpsdb_df_opt = optimise_dataframe(gpsdb_df_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the raw data and database data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLONASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import an example raw data file. Note that one of the NMEA strings, GPRMC has 30 columns which would need to be included if the full data set is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_data_folder_glonass = '/home/jen/projects/ace_data_management/ship_data/gps_bridge1/'\n",
    "glonass_filename = 'gpsdata-20170104.log'\n",
    "\n",
    "#test_list_glonass_data_files = get_input_file(input_data_folder_glonass, glonass_filename)\n",
    "test_list_glonass_data_files = ['/home/jen/projects/ace_data_management/ship_data/gps_bridge1/gpsdata-20170104.log', '/home/jen/projects/ace_data_management/ship_data/gps_bridge1/gpsdata-20170105.log']\n",
    "\n",
    "glonass_df = read_files(test_list_glonass_data_files)\n",
    "glonass_df = glonass_df.rename(columns = {'a': 'nmea_reference'})\n",
    "\n",
    "len(glonass_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glonass_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put GPGGA data into a separate dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmea_string = '$GPGGA'\n",
    "\n",
    "gpgga_glonass_df = get_nmea_string_data(nmea_string, glonass_df, gpgga_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpgga_glonass_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put GPZDA data into a separate dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmea_string = '$GPZDA'\n",
    "\n",
    "gpzda_glonass_df = get_nmea_string_data(nmea_string, glonass_df, gpzda_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpzda_glonass_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put GPRMC data into a separate dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmea_string = '$GPRMC'\n",
    "\n",
    "gprmc_glonass_df = get_nmea_string_data(nmea_string, glonass_df, gprmc_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gprmc_glonass_df.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gprmc_glonass_df['fix_time'] = pandas.to_datetime(gprmc_glonass_df['fix_time'], format='%H%M%S').dt.time # this works\n",
    "#datetime.time(gprmc_glonass_df['fix_time']) #TODO get the time only\n",
    "\n",
    "\n",
    "\n",
    "#gprmc_glonass_df['fix_time'].datetime.time()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gprmc_glonass_df.iloc[:5]\n",
    "#gprmc_glonass_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the GLONASS data from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_glonass = 'select * from ship_data_gpggagpsfix where device_id=64;'\n",
    "\n",
    "db_connection = MySQLdb.connect(host = 'localhost', user = 'ace', passwd = 'ace',db = 'ace2016', port = 3306); \n",
    "\n",
    "glonassdb_df = get_data_from_database(query_glonass, db_connection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
